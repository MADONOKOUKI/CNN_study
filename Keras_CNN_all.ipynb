{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.topology.Layer'>\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, list_pictures, load_img\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.gridspec as gridspec\n",
    "import shutil\n",
    "from keras.utils import plot_model\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from scipy.misc import imresize\n",
    "from skimage import io\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CurryPowder', 'rice', 'onion', 'potato', 'carot']\n",
      "(1152, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# フォルダの中にある画像を順次読み込む\n",
    "# カテゴリーは0から始める\n",
    "\n",
    "#参考\n",
    "#https://qiita.com/supersaiakujin/items/fc54116df9ca6958a68d\n",
    "#http://testpy.hatenablog.com/entry/2017/06/02/001901\n",
    "#https://qiita.com/zaburo/items/0b9db87d0a52191b164b\n",
    "# 画像の反転と回転の実装、またはdata argumentation の実装を行う\n",
    "X = []\n",
    "Y = []\n",
    "name_class = []\n",
    "#下、後でコード量を減らす\n",
    "# 対象Aの画像\n",
    "rate = 0.2\n",
    "label = 0\n",
    "for dir in os.listdir(\"data/train\"):\n",
    "#     print(dir)\n",
    "    if dir == \".DS_Store\":\n",
    "        continue\n",
    "    name_class.append(dir)\n",
    "    for picture in list_pictures(\"data/train/\"+dir):\n",
    "#         print(picture)\n",
    "    #     img =load_img(picture, target_size=(64,64))\n",
    "        img =imresize(skimage.io.imread(picture),(224,224), interp='nearest')\n",
    "#         img =imresize(skimage.io.imread(picture),(64,64), interp='nearest')\n",
    "    #     img_raw = skimage.io.imread\n",
    "#         print(img_to_array(skimage.transform.rotate(img, angle=0, resize=False, center=None)).shape)\n",
    "        for i in range(358,360):\n",
    "            #回転のみ\n",
    "            X.append(img_to_array(skimage.transform.rotate(img, angle=i, resize=False, center=None)))\n",
    "            Y.append(label )\n",
    "#             #回転と上下の反転\n",
    "            X.append(img_to_array(cv2.flip(skimage.transform.rotate(img, angle=i, resize=False, center=None),0)))\n",
    "            Y.append(label )\n",
    "#             #回転と左右の反転\n",
    "#             X.append(img_to_array(cv2.flip(skimage.transform.rotate(img, angle=i, resize=False, center=None),1)))\n",
    "#             Y.append(label )\n",
    "#             #拡大\n",
    "            X.append(img_to_array(skimage.transform.warp(skimage.transform.rotate(img, angle=i, resize=False, center=None), skimage.transform.AffineTransform(scale=(1-rate, 1-rate), translation=(64,64)))))\n",
    "            Y.append(label )\n",
    "    label += 1\n",
    "\n",
    "# # arrayに変換\n",
    "# X.shape()\n",
    "print(name_class)\n",
    "X = np.asarray(X)\n",
    "print(X.shape)\n",
    "# print(X)\n",
    "Y = np.asarray(Y)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画素値を0から1の範囲に変換\n",
    "X = X.astype('float32')\n",
    "X = X / 255.0\n",
    "\n",
    "# クラスの形式を変換\n",
    "Y = np_utils.to_categorical(Y, 5)\n",
    "\n",
    "# 学習用データとテストデータ\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNNを構築(kaggleコンペで高かった組み方)\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                  input_shape=X_train.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Conv2D(32, (3, 3)))\n",
    "# # model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# # model.add(Activation('relu'))\n",
    "# # model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(5))       # クラスは2個\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# # コンパイル\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='SGD',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # 実行。出力はなしで設定(verbose=0)。\n",
    "# history = model.fit(X_train, y_train, batch_size=5, epochs=50,\n",
    "#                    validation_data = (X_test, y_test), verbose = 1)\n",
    "\n",
    "# # plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "# # https://qiita.com/agumon/items/ab2de98a3783e0a93e66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 134,281,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1036 samples, validate on 116 samples\n",
      "Epoch 1/5\n",
      "\n",
      "   5/1036 [..............................] - ETA: 1:07:37 - loss: 1.7445 - acc: 0.2000\n",
      "  10/1036 [..............................] - ETA: 57:28 - loss: 4.8829 - acc: 0.2000  \n",
      "  15/1036 [..............................] - ETA: 53:44 - loss: 6.7163 - acc: 0.1333\n",
      "  20/1036 [..............................] - ETA: 51:47 - loss: 9.0667 - acc: 0.1000\n",
      "  25/1036 [..............................] - ETA: 50:27 - loss: 10.4770 - acc: 0.0800\n",
      "  30/1036 [..............................] - ETA: 49:36 - loss: 10.8799 - acc: 0.1000\n",
      "  35/1036 [>.............................] - ETA: 48:54 - loss: 11.6282 - acc: 0.0857\n",
      "  40/1036 [>.............................] - ETA: 48:23 - loss: 12.1895 - acc: 0.0750\n",
      "  45/1036 [>.............................] - ETA: 47:49 - loss: 11.9096 - acc: 0.1111\n",
      "  50/1036 [>.............................] - ETA: 47:20 - loss: 12.0081 - acc: 0.1200\n",
      "  55/1036 [>.............................] - ETA: 46:55 - loss: 12.3817 - acc: 0.1091\n",
      "  60/1036 [>.............................] - ETA: 46:34 - loss: 12.6931 - acc: 0.1000\n",
      "  65/1036 [>.............................] - ETA: 46:09 - loss: 12.4606 - acc: 0.1231\n",
      "  70/1036 [=>............................] - ETA: 45:53 - loss: 12.4916 - acc: 0.1286\n",
      "  75/1036 [=>............................] - ETA: 45:43 - loss: 12.5185 - acc: 0.1333\n",
      "  80/1036 [=>............................] - ETA: 45:31 - loss: 12.5420 - acc: 0.1375\n",
      "  85/1036 [=>............................] - ETA: 45:24 - loss: 12.7523 - acc: 0.1294\n",
      "  90/1036 [=>............................] - ETA: 45:36 - loss: 12.7602 - acc: 0.1333\n",
      "  95/1036 [=>............................] - ETA: 46:09 - loss: 12.5976 - acc: 0.1474\n",
      " 100/1036 [=>............................] - ETA: 46:10 - loss: 12.6125 - acc: 0.1500\n",
      " 105/1036 [==>...........................] - ETA: 46:32 - loss: 12.6259 - acc: 0.1524\n",
      " 110/1036 [==>...........................] - ETA: 47:01 - loss: 12.7846 - acc: 0.1455\n",
      " 115/1036 [==>...........................] - ETA: 47:21 - loss: 12.9296 - acc: 0.1391\n",
      " 120/1036 [==>...........................] - ETA: 47:38 - loss: 12.9281 - acc: 0.1417\n",
      " 125/1036 [==>...........................] - ETA: 47:37 - loss: 12.9268 - acc: 0.1440\n",
      " 130/1036 [==>...........................] - ETA: 47:19 - loss: 12.9255 - acc: 0.1462\n",
      " 135/1036 [==>...........................] - ETA: 47:10 - loss: 13.0438 - acc: 0.1407\n",
      " 140/1036 [===>..........................] - ETA: 47:53 - loss: 12.9233 - acc: 0.1500\n",
      " 145/1036 [===>..........................] - ETA: 48:16 - loss: 12.8111 - acc: 0.1586\n",
      " 150/1036 [===>..........................] - ETA: 48:27 - loss: 12.8139 - acc: 0.1600\n",
      " 155/1036 [===>..........................] - ETA: 48:25 - loss: 12.8165 - acc: 0.1613\n",
      " 160/1036 [===>..........................] - ETA: 48:31 - loss: 12.9197 - acc: 0.1563\n",
      " 165/1036 [===>..........................] - ETA: 48:24 - loss: 12.9189 - acc: 0.1576\n",
      " 170/1036 [===>..........................] - ETA: 48:17 - loss: 12.9182 - acc: 0.1588\n",
      " 175/1036 [====>.........................] - ETA: 48:04 - loss: 12.9175 - acc: 0.1600\n",
      " 180/1036 [====>.........................] - ETA: 47:52 - loss: 12.8273 - acc: 0.1667\n",
      " 185/1036 [====>.........................] - ETA: 47:39 - loss: 12.8292 - acc: 0.1676\n",
      " 190/1036 [====>.........................] - ETA: 47:22 - loss: 12.8309 - acc: 0.1684\n",
      " 195/1036 [====>.........................] - ETA: 47:14 - loss: 12.7499 - acc: 0.1744\n",
      " 200/1036 [====>.........................] - ETA: 47:05 - loss: 12.7535 - acc: 0.1750\n",
      " 205/1036 [====>.........................] - ETA: 46:55 - loss: 12.7569 - acc: 0.1756\n",
      " 210/1036 [=====>........................] - ETA: 46:26 - loss: 12.7602 - acc: 0.1762\n",
      " 215/1036 [=====>........................] - ETA: 46:09 - loss: 12.8383 - acc: 0.1721\n",
      " 220/1036 [=====>........................] - ETA: 45:39 - loss: 12.6930 - acc: 0.1818\n",
      " 225/1036 [=====>........................] - ETA: 45:11 - loss: 12.5542 - acc: 0.1911\n",
      " 230/1036 [=====>........................] - ETA: 44:46 - loss: 12.4915 - acc: 0.1957\n",
      " 235/1036 [=====>........................] - ETA: 44:20 - loss: 12.5001 - acc: 0.1957\n",
      " 240/1036 [=====>........................] - ETA: 43:53 - loss: 12.5083 - acc: 0.1958\n",
      " 245/1036 [======>.......................] - ETA: 43:27 - loss: 12.5162 - acc: 0.1959\n",
      " 250/1036 [======>.......................] - ETA: 43:03 - loss: 12.4593 - acc: 0.2000\n",
      " 255/1036 [======>.......................] - ETA: 42:38 - loss: 12.4046 - acc: 0.2039\n",
      " 260/1036 [======>.......................] - ETA: 42:22 - loss: 12.4141 - acc: 0.2038\n",
      " 265/1036 [======>.......................] - ETA: 42:12 - loss: 12.4839 - acc: 0.2000\n",
      " 270/1036 [======>.......................] - ETA: 41:48 - loss: 12.4915 - acc: 0.2000\n",
      " 275/1036 [======>.......................] - ETA: 41:24 - loss: 12.5575 - acc: 0.1964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 280/1036 [=======>......................] - ETA: 41:02 - loss: 12.5635 - acc: 0.1964\n",
      " 285/1036 [=======>......................] - ETA: 40:52 - loss: 12.6259 - acc: 0.1930\n",
      " 290/1036 [=======>......................] - ETA: 40:38 - loss: 12.6861 - acc: 0.1897\n",
      " 295/1036 [=======>......................] - ETA: 40:15 - loss: 12.6896 - acc: 0.1898\n",
      " 300/1036 [=======>......................] - ETA: 39:52 - loss: 12.6393 - acc: 0.1933\n",
      " 305/1036 [=======>......................] - ETA: 39:29 - loss: 12.6435 - acc: 0.1934\n",
      " 310/1036 [=======>......................] - ETA: 39:07 - loss: 12.6475 - acc: 0.1935\n",
      " 315/1036 [========>.....................] - ETA: 38:45 - loss: 12.6003 - acc: 0.1968\n",
      " 320/1036 [========>.....................] - ETA: 38:23 - loss: 12.6049 - acc: 0.1969\n",
      " 325/1036 [========>.....................] - ETA: 38:03 - loss: 12.6093 - acc: 0.1969\n",
      " 330/1036 [========>.....................] - ETA: 37:42 - loss: 12.6625 - acc: 0.1939\n",
      " 335/1036 [========>.....................] - ETA: 37:25 - loss: 12.6178 - acc: 0.1970\n",
      " 340/1036 [========>.....................] - ETA: 37:07 - loss: 12.5745 - acc: 0.2000\n",
      " 345/1036 [========>.....................] - ETA: 36:53 - loss: 12.4857 - acc: 0.2058\n",
      " 350/1036 [=========>....................] - ETA: 36:44 - loss: 12.4455 - acc: 0.2086\n",
      " 355/1036 [=========>....................] - ETA: 36:34 - loss: 12.4972 - acc: 0.2056\n",
      " 360/1036 [=========>....................] - ETA: 36:24 - loss: 12.5027 - acc: 0.2056\n",
      " 365/1036 [=========>....................] - ETA: 36:23 - loss: 12.5523 - acc: 0.2027\n",
      " 370/1036 [=========>....................] - ETA: 36:11 - loss: 12.5569 - acc: 0.2027\n",
      " 375/1036 [=========>....................] - ETA: 35:56 - loss: 12.5614 - acc: 0.2027\n",
      " 380/1036 [==========>...................] - ETA: 35:42 - loss: 12.5658 - acc: 0.2026\n",
      " 385/1036 [==========>...................] - ETA: 35:34 - loss: 12.5700 - acc: 0.2026\n",
      " 390/1036 [==========>...................] - ETA: 35:15 - loss: 12.5742 - acc: 0.2026\n",
      " 395/1036 [==========>...................] - ETA: 34:56 - loss: 12.6191 - acc: 0.2000\n",
      " 400/1036 [==========>...................] - ETA: 34:36 - loss: 12.6225 - acc: 0.2000\n",
      " 405/1036 [==========>...................] - ETA: 34:16 - loss: 12.6259 - acc: 0.2000\n",
      " 410/1036 [==========>...................] - ETA: 34:01 - loss: 12.6291 - acc: 0.2000\n",
      " 415/1036 [===========>..................] - ETA: 33:47 - loss: 12.6323 - acc: 0.2000\n",
      " 420/1036 [===========>..................] - ETA: 33:36 - loss: 12.6354 - acc: 0.2000\n",
      " 425/1036 [===========>..................] - ETA: 33:29 - loss: 12.6385 - acc: 0.2000\n",
      " 430/1036 [===========>..................] - ETA: 33:18 - loss: 12.6040 - acc: 0.2023\n",
      " 435/1036 [===========>..................] - ETA: 33:06 - loss: 12.5703 - acc: 0.2046\n",
      " 440/1036 [===========>..................] - ETA: 32:47 - loss: 12.5740 - acc: 0.2045\n",
      " 445/1036 [===========>..................] - ETA: 32:28 - loss: 12.6138 - acc: 0.2022\n",
      " 450/1036 [============>.................] - ETA: 32:14 - loss: 12.6169 - acc: 0.2022\n",
      " 455/1036 [============>.................] - ETA: 32:01 - loss: 12.6554 - acc: 0.2000\n",
      " 460/1036 [============>.................] - ETA: 31:44 - loss: 12.6580 - acc: 0.2000\n",
      " 465/1036 [============>.................] - ETA: 31:27 - loss: 12.6952 - acc: 0.1978\n",
      " 470/1036 [============>.................] - ETA: 31:08 - loss: 12.6287 - acc: 0.2021\n",
      " 475/1036 [============>.................] - ETA: 30:51 - loss: 12.6315 - acc: 0.2021\n",
      " 480/1036 [============>.................] - ETA: 30:36 - loss: 12.6007 - acc: 0.2042\n",
      " 485/1036 [=============>................] - ETA: 30:17 - loss: 12.5705 - acc: 0.2062\n",
      " 490/1036 [=============>................] - ETA: 29:58 - loss: 12.5738 - acc: 0.2061\n",
      " 495/1036 [=============>................] - ETA: 29:38 - loss: 12.5770 - acc: 0.2061\n",
      " 500/1036 [=============>................] - ETA: 29:19 - loss: 12.5479 - acc: 0.2080\n",
      " 505/1036 [=============>................] - ETA: 28:59 - loss: 12.5833 - acc: 0.2059\n",
      " 510/1036 [=============>................] - ETA: 28:40 - loss: 12.5863 - acc: 0.2059\n",
      " 515/1036 [=============>................] - ETA: 28:21 - loss: 12.5893 - acc: 0.2058\n",
      " 520/1036 [==============>...............] - ETA: 28:03 - loss: 12.6233 - acc: 0.2038\n",
      " 525/1036 [==============>...............] - ETA: 27:44 - loss: 12.6259 - acc: 0.2038\n",
      " 530/1036 [==============>...............] - ETA: 27:25 - loss: 12.6284 - acc: 0.2038\n",
      " 535/1036 [==============>...............] - ETA: 27:08 - loss: 12.5706 - acc: 0.2075\n",
      " 540/1036 [==============>...............] - ETA: 26:49 - loss: 12.5736 - acc: 0.2074\n",
      " 545/1036 [==============>...............] - ETA: 26:32 - loss: 12.5174 - acc: 0.2110\n",
      " 550/1036 [==============>...............] - ETA: 26:18 - loss: 12.5208 - acc: 0.2109\n",
      " 555/1036 [===============>..............] - ETA: 26:04 - loss: 12.5242 - acc: 0.2108\n",
      " 560/1036 [===============>..............] - ETA: 25:48 - loss: 12.5275 - acc: 0.2107\n",
      " 565/1036 [===============>..............] - ETA: 25:31 - loss: 12.5593 - acc: 0.2088\n",
      " 570/1036 [===============>..............] - ETA: 25:16 - loss: 12.5622 - acc: 0.2088\n",
      " 575/1036 [===============>..............] - ETA: 25:02 - loss: 12.5931 - acc: 0.2070\n",
      " 580/1036 [===============>..............] - ETA: 24:47 - loss: 12.5957 - acc: 0.2069\n",
      " 585/1036 [===============>..............] - ETA: 24:32 - loss: 12.5983 - acc: 0.2068\n",
      " 590/1036 [================>.............] - ETA: 24:19 - loss: 12.6008 - acc: 0.2068\n",
      " 595/1036 [================>.............] - ETA: 24:03 - loss: 12.6033 - acc: 0.2067\n",
      " 600/1036 [================>.............] - ETA: 23:45 - loss: 12.6057 - acc: 0.2067\n",
      " 605/1036 [================>.............] - ETA: 23:29 - loss: 12.6347 - acc: 0.2050\n",
      " 610/1036 [================>.............] - ETA: 23:13 - loss: 12.6633 - acc: 0.2033\n",
      " 615/1036 [================>.............] - ETA: 22:58 - loss: 12.6652 - acc: 0.2033\n",
      " 620/1036 [================>.............] - ETA: 22:45 - loss: 12.6670 - acc: 0.2032\n",
      " 625/1036 [=================>............] - ETA: 22:27 - loss: 12.6688 - acc: 0.2032\n",
      " 630/1036 [=================>............] - ETA: 22:10 - loss: 12.6450 - acc: 0.2048\n",
      " 635/1036 [=================>............] - ETA: 21:54 - loss: 12.5709 - acc: 0.2094\n",
      " 640/1036 [=================>............] - ETA: 21:38 - loss: 12.5986 - acc: 0.2078\n",
      " 645/1036 [=================>............] - ETA: 21:21 - loss: 12.6009 - acc: 0.2078\n",
      " 650/1036 [=================>............] - ETA: 21:07 - loss: 12.6279 - acc: 0.2062\n",
      " 655/1036 [=================>............] - ETA: 20:53 - loss: 12.6300 - acc: 0.2061\n",
      " 660/1036 [==================>...........] - ETA: 20:39 - loss: 12.6320 - acc: 0.2061\n",
      " 665/1036 [==================>...........] - ETA: 20:23 - loss: 12.6339 - acc: 0.2060\n",
      " 670/1036 [==================>...........] - ETA: 20:08 - loss: 12.6359 - acc: 0.2060\n",
      " 675/1036 [==================>...........] - ETA: 19:51 - loss: 12.6617 - acc: 0.2044\n",
      " 680/1036 [==================>...........] - ETA: 19:34 - loss: 12.6634 - acc: 0.2044\n",
      " 685/1036 [==================>...........] - ETA: 19:16 - loss: 12.6651 - acc: 0.2044\n",
      " 690/1036 [==================>...........] - ETA: 18:58 - loss: 12.6667 - acc: 0.2043\n",
      " 695/1036 [===================>..........] - ETA: 18:40 - loss: 12.5988 - acc: 0.2086\n",
      " 700/1036 [===================>..........] - ETA: 18:22 - loss: 12.6009 - acc: 0.2086\n",
      " 705/1036 [===================>..........] - ETA: 18:05 - loss: 12.5801 - acc: 0.2099\n",
      " 710/1036 [===================>..........] - ETA: 17:47 - loss: 12.5823 - acc: 0.2099\n",
      " 715/1036 [===================>..........] - ETA: 17:29 - loss: 12.5620 - acc: 0.2112\n",
      " 720/1036 [===================>..........] - ETA: 17:12 - loss: 12.5643 - acc: 0.2111\n",
      " 725/1036 [===================>..........] - ETA: 16:54 - loss: 12.5443 - acc: 0.2124\n",
      " 730/1036 [====================>.........] - ETA: 16:37 - loss: 12.5688 - acc: 0.2110\n",
      " 735/1036 [====================>.........] - ETA: 16:20 - loss: 12.5930 - acc: 0.2095\n",
      " 740/1036 [====================>.........] - ETA: 16:02 - loss: 12.6168 - acc: 0.2081\n",
      " 745/1036 [====================>.........] - ETA: 15:45 - loss: 12.6403 - acc: 0.2067\n",
      " 750/1036 [====================>.........] - ETA: 15:28 - loss: 12.6205 - acc: 0.2080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 755/1036 [====================>.........] - ETA: 15:11 - loss: 12.6223 - acc: 0.2079\n",
      " 760/1036 [=====================>........] - ETA: 14:54 - loss: 12.6241 - acc: 0.2079\n",
      " 765/1036 [=====================>........] - ETA: 14:37 - loss: 12.6048 - acc: 0.2092\n",
      " 770/1036 [=====================>........] - ETA: 14:19 - loss: 12.6276 - acc: 0.2078\n",
      " 775/1036 [=====================>........] - ETA: 14:03 - loss: 12.6501 - acc: 0.2065\n",
      " 780/1036 [=====================>........] - ETA: 13:46 - loss: 12.6310 - acc: 0.2077\n",
      " 785/1036 [=====================>........] - ETA: 13:29 - loss: 12.6122 - acc: 0.2089\n",
      " 790/1036 [=====================>........] - ETA: 13:12 - loss: 12.6139 - acc: 0.2089\n",
      " 795/1036 [======================>.......] - ETA: 12:55 - loss: 12.6360 - acc: 0.2075\n",
      " 800/1036 [======================>.......] - ETA: 12:38 - loss: 12.6376 - acc: 0.2075\n",
      " 805/1036 [======================>.......] - ETA: 12:22 - loss: 12.5992 - acc: 0.2099\n",
      " 810/1036 [======================>.......] - ETA: 12:05 - loss: 12.6010 - acc: 0.2099\n",
      " 815/1036 [======================>.......] - ETA: 11:48 - loss: 12.6028 - acc: 0.2098\n",
      " 820/1036 [======================>.......] - ETA: 11:32 - loss: 12.6046 - acc: 0.2098\n",
      " 825/1036 [======================>.......] - ETA: 11:15 - loss: 12.5672 - acc: 0.2121\n",
      " 830/1036 [=======================>......] - ETA: 11:00 - loss: 12.5692 - acc: 0.2120\n",
      " 835/1036 [=======================>......] - ETA: 10:44 - loss: 12.5905 - acc: 0.2108\n",
      " 840/1036 [=======================>......] - ETA: 10:27 - loss: 12.5923 - acc: 0.2107\n",
      " 845/1036 [=======================>......] - ETA: 10:11 - loss: 12.5941 - acc: 0.2107\n",
      " 850/1036 [=======================>......] - ETA: 9:54 - loss: 12.5958 - acc: 0.2106 \n",
      " 855/1036 [=======================>......] - ETA: 9:38 - loss: 12.5976 - acc: 0.2105\n",
      " 860/1036 [=======================>......] - ETA: 9:21 - loss: 12.5806 - acc: 0.2116\n",
      " 865/1036 [========================>.....] - ETA: 9:05 - loss: 12.5824 - acc: 0.2116\n",
      " 870/1036 [========================>.....] - ETA: 8:48 - loss: 12.6027 - acc: 0.2103\n",
      " 875/1036 [========================>.....] - ETA: 8:32 - loss: 12.6044 - acc: 0.2103\n",
      " 880/1036 [========================>.....] - ETA: 8:16 - loss: 12.6243 - acc: 0.2091\n",
      " 885/1036 [========================>.....] - ETA: 8:00 - loss: 12.6258 - acc: 0.2090\n",
      " 890/1036 [========================>.....] - ETA: 7:44 - loss: 12.6092 - acc: 0.2101\n",
      " 895/1036 [========================>.....] - ETA: 7:29 - loss: 12.5928 - acc: 0.2112\n",
      " 900/1036 [=========================>....] - ETA: 7:13 - loss: 12.6124 - acc: 0.2100\n",
      " 905/1036 [=========================>....] - ETA: 6:58 - loss: 12.5962 - acc: 0.2110\n",
      " 910/1036 [=========================>....] - ETA: 6:43 - loss: 12.5978 - acc: 0.2110\n",
      " 915/1036 [=========================>....] - ETA: 6:27 - loss: 12.5818 - acc: 0.2120\n",
      " 920/1036 [=========================>....] - ETA: 6:11 - loss: 12.5835 - acc: 0.2120\n",
      " 925/1036 [=========================>....] - ETA: 5:55 - loss: 12.6026 - acc: 0.2108\n",
      " 930/1036 [=========================>....] - ETA: 5:39 - loss: 12.6042 - acc: 0.2108\n",
      " 935/1036 [==========================>...] - ETA: 5:23 - loss: 12.6230 - acc: 0.2096\n",
      " 940/1036 [==========================>...] - ETA: 5:07 - loss: 12.6244 - acc: 0.2096\n",
      " 945/1036 [==========================>...] - ETA: 4:52 - loss: 12.6258 - acc: 0.2095\n",
      " 950/1036 [==========================>...] - ETA: 4:36 - loss: 12.6273 - acc: 0.2095\n",
      " 955/1036 [==========================>...] - ETA: 4:20 - loss: 12.6455 - acc: 0.2084\n",
      " 960/1036 [==========================>...] - ETA: 4:04 - loss: 12.6468 - acc: 0.2083\n",
      " 965/1036 [==========================>...] - ETA: 3:48 - loss: 12.6648 - acc: 0.2073\n",
      " 970/1036 [===========================>..] - ETA: 3:32 - loss: 12.6660 - acc: 0.2072\n",
      " 975/1036 [===========================>..] - ETA: 3:16 - loss: 12.6672 - acc: 0.2072\n",
      " 980/1036 [===========================>..] - ETA: 3:00 - loss: 12.6354 - acc: 0.2092\n",
      " 985/1036 [===========================>..] - ETA: 2:44 - loss: 12.6368 - acc: 0.2091\n",
      " 990/1036 [===========================>..] - ETA: 2:28 - loss: 12.6218 - acc: 0.2101\n",
      " 995/1036 [===========================>..] - ETA: 2:12 - loss: 12.6231 - acc: 0.2101\n",
      "1000/1036 [===========================>..] - ETA: 1:56 - loss: 12.6084 - acc: 0.2110\n",
      "1005/1036 [============================>.] - ETA: 1:40 - loss: 12.6098 - acc: 0.2109\n",
      "1010/1036 [============================>.] - ETA: 1:24 - loss: 12.6112 - acc: 0.2109\n",
      "1015/1036 [============================>.] - ETA: 1:08 - loss: 12.5967 - acc: 0.2118\n",
      "1020/1036 [============================>.] - ETA: 1:20 - loss: 12.5666 - acc: 0.2137\n",
      "1025/1036 [============================>.] - ETA: 55s - loss: 12.5525 - acc: 0.2146 \n",
      "1030/1036 [============================>.] - ETA: 29s - loss: 12.5698 - acc: 0.2136\n",
      "1035/1036 [============================>.] - ETA: 4s - loss: 12.5713 - acc: 0.2135 \n",
      "1036/1036 [==============================] - ETA: 0s - loss: 12.5592 - acc: 0.2143 - val_loss: 12.2275 - val_acc: 0.2414\n",
      "Epoch 2/5\n",
      "\n",
      "   5/1036 [..............................] - ETA: 1:03:40 - loss: 6.4472 - acc: 0.6000\n",
      "  10/1036 [..............................] - ETA: 49:32:09 - loss: 11.2827 - acc: 0.3000\n",
      "  15/1036 [..............................] - ETA: 33:13:00 - loss: 10.7454 - acc: 0.3333\n",
      "  20/1036 [..............................] - ETA: 53:42:40 - loss: 10.4768 - acc: 0.3500\n",
      "  25/1036 [..............................] - ETA: 42:56:50 - loss: 10.9603 - acc: 0.3200\n",
      "  30/1036 [..............................] - ETA: 35:47:28 - loss: 11.8199 - acc: 0.2667\n",
      "  35/1036 [>.............................] - ETA: 49:28:10 - loss: 11.9734 - acc: 0.2571\n",
      "  40/1036 [>.............................] - ETA: 43:11:03 - loss: 11.6856 - acc: 0.2750\n",
      "  45/1036 [>.............................] - ETA: 38:18:40 - loss: 11.8199 - acc: 0.2667\n",
      "  50/1036 [>.............................] - ETA: 44:53:56 - loss: 11.2827 - acc: 0.3000\n",
      "  55/1036 [>.............................] - ETA: 40:42:35 - loss: 11.7223 - acc: 0.2727\n",
      "  60/1036 [>.............................] - ETA: 37:12:43 - loss: 11.8199 - acc: 0.2667\n",
      "  65/1036 [>.............................] - ETA: 35:22:47 - loss: 11.6546 - acc: 0.2769\n",
      "  70/1036 [=>............................] - ETA: 32:54:47 - loss: 11.7432 - acc: 0.2714\n",
      "  75/1036 [=>............................] - ETA: 30:38:04 - loss: 12.0348 - acc: 0.2533\n",
      "  80/1036 [=>............................] - ETA: 28:37:20 - loss: 11.6856 - acc: 0.2750\n",
      "  85/1036 [=>............................] - ETA: 26:51:07 - loss: 11.5671 - acc: 0.2824\n",
      "  90/1036 [=>............................] - ETA: 25:16:18 - loss: 11.6408 - acc: 0.2778\n",
      "  95/1036 [=>............................] - ETA: 23:51:31 - loss: 11.5372 - acc: 0.2842\n",
      " 100/1036 [=>............................] - ETA: 22:34:48 - loss: 11.7662 - acc: 0.2700\n",
      " 105/1036 [==>...........................] - ETA: 21:25:24 - loss: 11.6664 - acc: 0.2762\n",
      " 110/1036 [==>...........................] - ETA: 20:22:18 - loss: 11.7223 - acc: 0.2727\n",
      " 115/1036 [==>...........................] - ETA: 19:24:39 - loss: 11.7732 - acc: 0.2696\n",
      " 120/1036 [==>...........................] - ETA: 18:31:49 - loss: 11.5513 - acc: 0.2833\n",
      " 125/1036 [==>...........................] - ETA: 17:43:10 - loss: 11.6050 - acc: 0.2800\n",
      " 130/1036 [==>...........................] - ETA: 16:58:14 - loss: 11.7786 - acc: 0.2692\n",
      " 135/1036 [==>...........................] - ETA: 16:16:39 - loss: 11.7005 - acc: 0.2741\n",
      " 140/1036 [===>..........................] - ETA: 15:38:00 - loss: 11.8583 - acc: 0.2643\n",
      " 145/1036 [===>..........................] - ETA: 15:02:00 - loss: 12.0052 - acc: 0.2552\n",
      " 150/1036 [===>..........................] - ETA: 14:28:23 - loss: 12.1423 - acc: 0.2467\n",
      " 155/1036 [===>..........................] - ETA: 13:56:55 - loss: 11.9586 - acc: 0.2581\n",
      " 160/1036 [===>..........................] - ETA: 13:27:24 - loss: 11.8871 - acc: 0.2625\n",
      " 165/1036 [===>..........................] - ETA: 12:59:40 - loss: 11.8199 - acc: 0.2667\n",
      " 170/1036 [===>..........................] - ETA: 12:33:47 - loss: 11.8515 - acc: 0.2647\n",
      " 175/1036 [====>.........................] - ETA: 12:09:32 - loss: 11.6971 - acc: 0.2743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 180/1036 [====>.........................] - ETA: 11:46:18 - loss: 11.7304 - acc: 0.2722\n",
      " 185/1036 [====>.........................] - ETA: 11:24:16 - loss: 11.7619 - acc: 0.2703\n",
      " 190/1036 [====>.........................] - ETA: 11:03:27 - loss: 11.7917 - acc: 0.2684\n",
      " 195/1036 [====>.........................] - ETA: 10:43:38 - loss: 11.7373 - acc: 0.2718\n",
      " 200/1036 [====>.........................] - ETA: 10:24:46 - loss: 11.6856 - acc: 0.2750\n",
      " 205/1036 [====>.........................] - ETA: 10:06:50 - loss: 11.7151 - acc: 0.2732\n",
      " 210/1036 [=====>........................] - ETA: 9:49:43 - loss: 11.8199 - acc: 0.2667 \n",
      " 215/1036 [=====>........................] - ETA: 9:33:24 - loss: 11.9199 - acc: 0.2605\n",
      " 220/1036 [=====>........................] - ETA: 9:17:52 - loss: 11.8688 - acc: 0.2636\n",
      " 225/1036 [=====>........................] - ETA: 9:03:12 - loss: 11.8916 - acc: 0.2622\n",
      " 230/1036 [=====>........................] - ETA: 8:49:17 - loss: 11.9134 - acc: 0.2609\n",
      " 235/1036 [=====>........................] - ETA: 8:35:41 - loss: 11.8657 - acc: 0.2638\n",
      " 240/1036 [=====>........................] - ETA: 8:22:35 - loss: 11.8871 - acc: 0.2625\n",
      " 245/1036 [======>.......................] - ETA: 8:09:58 - loss: 11.8419 - acc: 0.2653\n",
      " 250/1036 [======>.......................] - ETA: 7:58:01 - loss: 11.7984 - acc: 0.2680\n",
      " 255/1036 [======>.......................] - ETA: 7:46:27 - loss: 11.8199 - acc: 0.2667\n",
      " 260/1036 [======>.......................] - ETA: 7:35:17 - loss: 11.9026 - acc: 0.2615\n",
      " 265/1036 [======>.......................] - ETA: 7:24:30 - loss: 11.9213 - acc: 0.2604\n",
      " 270/1036 [======>.......................] - ETA: 7:14:07 - loss: 11.9990 - acc: 0.2556\n",
      " 275/1036 [======>.......................] - ETA: 7:04:06 - loss: 12.0739 - acc: 0.2509\n",
      " 280/1036 [=======>......................] - ETA: 6:54:26 - loss: 12.0886 - acc: 0.2500\n",
      " 285/1036 [=======>......................] - ETA: 6:45:08 - loss: 12.1027 - acc: 0.2491\n",
      " 290/1036 [=======>......................] - ETA: 6:36:07 - loss: 12.1719 - acc: 0.2448\n",
      " 295/1036 [=======>......................] - ETA: 6:27:24 - loss: 12.1842 - acc: 0.2441\n",
      " 300/1036 [=======>......................] - ETA: 6:18:57 - loss: 12.1960 - acc: 0.2433\n",
      " 305/1036 [=======>......................] - ETA: 6:10:47 - loss: 12.2075 - acc: 0.2426\n",
      " 310/1036 [=======>......................] - ETA: 6:02:56 - loss: 12.2705 - acc: 0.2387\n",
      " 315/1036 [========>.....................] - ETA: 5:55:18 - loss: 12.2805 - acc: 0.2381\n",
      " 320/1036 [========>.....................] - ETA: 5:47:52 - loss: 12.2900 - acc: 0.2375\n",
      " 325/1036 [========>.....................] - ETA: 5:40:40 - loss: 12.2498 - acc: 0.2400\n",
      " 330/1036 [========>.....................] - ETA: 5:33:41 - loss: 12.2595 - acc: 0.2394\n",
      " 335/1036 [========>.....................] - ETA: 5:26:54 - loss: 12.2690 - acc: 0.2388\n",
      " 340/1036 [========>.....................] - ETA: 5:20:17 - loss: 12.2782 - acc: 0.2382\n",
      " 345/1036 [========>.....................] - ETA: 5:13:51 - loss: 12.1470 - acc: 0.2464\n",
      " 350/1036 [=========>....................] - ETA: 5:07:36 - loss: 12.2037 - acc: 0.2429\n",
      " 355/1036 [=========>....................] - ETA: 5:01:31 - loss: 12.1226 - acc: 0.2479\n",
      " 360/1036 [=========>....................] - ETA: 4:55:36 - loss: 12.1333 - acc: 0.2472\n",
      " 365/1036 [=========>....................] - ETA: 4:49:52 - loss: 12.1879 - acc: 0.2438\n",
      " 370/1036 [=========>....................] - ETA: 4:44:15 - loss: 12.2410 - acc: 0.2405\n",
      " 375/1036 [=========>....................] - ETA: 4:38:47 - loss: 12.2927 - acc: 0.2373\n",
      " 380/1036 [==========>...................] - ETA: 4:33:27 - loss: 12.2158 - acc: 0.2421\n",
      " 385/1036 [==========>...................] - ETA: 4:28:14 - loss: 12.2665 - acc: 0.2390\n",
      " 390/1036 [==========>...................] - ETA: 4:23:08 - loss: 12.3159 - acc: 0.2359\n",
      " 395/1036 [==========>...................] - ETA: 4:18:10 - loss: 12.3640 - acc: 0.2329\n",
      " 400/1036 [==========>...................] - ETA: 4:13:20 - loss: 12.4109 - acc: 0.2300\n",
      " 405/1036 [==========>...................] - ETA: 4:08:36 - loss: 12.4169 - acc: 0.2296\n",
      " 410/1036 [==========>...................] - ETA: 4:03:59 - loss: 12.3834 - acc: 0.2317\n",
      " 415/1036 [===========>..................] - ETA: 3:59:27 - loss: 12.4284 - acc: 0.2289\n",
      " 420/1036 [===========>..................] - ETA: 3:55:03 - loss: 12.3956 - acc: 0.2310\n",
      " 425/1036 [===========>..................] - ETA: 3:50:44 - loss: 12.4394 - acc: 0.2282\n",
      " 430/1036 [===========>..................] - ETA: 3:46:31 - loss: 12.4822 - acc: 0.2256\n",
      " 435/1036 [===========>..................] - ETA: 3:42:23 - loss: 12.5239 - acc: 0.2230\n",
      " 440/1036 [===========>..................] - ETA: 3:38:20 - loss: 12.5648 - acc: 0.2205\n",
      " 445/1036 [===========>..................] - ETA: 3:34:23 - loss: 12.5685 - acc: 0.2202\n",
      " 450/1036 [============>.................] - ETA: 3:30:31 - loss: 12.5721 - acc: 0.2200\n",
      " 455/1036 [============>.................] - ETA: 3:26:43 - loss: 12.5402 - acc: 0.2220\n",
      " 460/1036 [============>.................] - ETA: 3:23:00 - loss: 12.5090 - acc: 0.2239\n",
      " 465/1036 [============>.................] - ETA: 3:19:23 - loss: 12.4785 - acc: 0.2258\n",
      " 470/1036 [============>.................] - ETA: 3:15:49 - loss: 12.4830 - acc: 0.2255\n",
      " 475/1036 [============>.................] - ETA: 3:12:19 - loss: 12.4873 - acc: 0.2253\n",
      " 480/1036 [============>.................] - ETA: 3:08:54 - loss: 12.4915 - acc: 0.2250\n",
      " 485/1036 [=============>................] - ETA: 3:05:36 - loss: 12.5289 - acc: 0.2227\n",
      " 490/1036 [=============>................] - ETA: 3:02:26 - loss: 12.5655 - acc: 0.2204\n",
      " 495/1036 [=============>................] - ETA: 2:59:15 - loss: 12.5363 - acc: 0.2222\n",
      " 500/1036 [=============>................] - ETA: 2:56:12 - loss: 12.5399 - acc: 0.2220\n",
      " 505/1036 [=============>................] - ETA: 2:53:09 - loss: 12.5434 - acc: 0.2218\n",
      " 510/1036 [=============>................] - ETA: 2:50:06 - loss: 12.5468 - acc: 0.2216\n",
      " 515/1036 [=============>................] - ETA: 2:47:05 - loss: 12.5189 - acc: 0.2233\n",
      " 520/1036 [==============>...............] - ETA: 2:44:08 - loss: 12.5225 - acc: 0.2231\n",
      " 525/1036 [==============>...............] - ETA: 2:41:14 - loss: 12.5568 - acc: 0.2210\n",
      " 530/1036 [==============>...............] - ETA: 2:38:23 - loss: 12.5904 - acc: 0.2189\n",
      " 535/1036 [==============>...............] - ETA: 2:35:34 - loss: 12.5932 - acc: 0.2187\n",
      " 540/1036 [==============>...............] - ETA: 2:32:54 - loss: 12.5960 - acc: 0.2185\n",
      " 545/1036 [==============>...............] - ETA: 3:47:18 - loss: 12.6283 - acc: 0.2165\n",
      " 550/1036 [==============>...............] - ETA: 4:06:36 - loss: 12.6307 - acc: 0.2164\n",
      " 555/1036 [===============>..............] - ETA: 4:02:12 - loss: 12.5750 - acc: 0.2198\n",
      " 560/1036 [===============>..............] - ETA: 3:58:19 - loss: 12.5779 - acc: 0.2196\n",
      " 565/1036 [===============>..............] - ETA: 3:55:59 - loss: 12.6092 - acc: 0.2177\n",
      " 570/1036 [===============>..............] - ETA: 3:51:40 - loss: 12.5834 - acc: 0.2193\n",
      " 575/1036 [===============>..............] - ETA: 3:48:12 - loss: 12.5861 - acc: 0.2191\n",
      " 580/1036 [===============>..............] - ETA: 3:44:05 - loss: 12.5610 - acc: 0.2207\n",
      " 585/1036 [===============>..............] - ETA: 3:41:13 - loss: 12.5638 - acc: 0.2205\n",
      " 590/1036 [================>.............] - ETA: 3:37:09 - loss: 12.5120 - acc: 0.2237\n",
      " 595/1036 [================>.............] - ETA: 3:33:47 - loss: 12.5152 - acc: 0.2235\n",
      " 600/1036 [================>.............] - ETA: 3:30:03 - loss: 12.5184 - acc: 0.2233\n",
      " 605/1036 [================>.............] - ETA: 3:26:08 - loss: 12.5481 - acc: 0.2215\n",
      " 610/1036 [================>.............] - ETA: 3:22:17 - loss: 12.5510 - acc: 0.2213\n",
      " 615/1036 [================>.............] - ETA: 3:18:28 - loss: 12.5800 - acc: 0.2195\n",
      " 620/1036 [================>.............] - ETA: 3:14:42 - loss: 12.5825 - acc: 0.2194\n",
      " 625/1036 [=================>............] - ETA: 3:10:59 - loss: 12.5850 - acc: 0.2192\n",
      " 630/1036 [=================>............] - ETA: 3:07:19 - loss: 12.6130 - acc: 0.2175\n",
      " 635/1036 [=================>............] - ETA: 3:03:43 - loss: 12.6406 - acc: 0.2157\n",
      " 640/1036 [=================>............] - ETA: 3:00:10 - loss: 12.6426 - acc: 0.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 645/1036 [=================>............] - ETA: 2:56:40 - loss: 12.6196 - acc: 0.2171\n",
      " 650/1036 [=================>............] - ETA: 2:53:13 - loss: 12.6465 - acc: 0.2154\n",
      " 655/1036 [=================>............] - ETA: 2:49:48 - loss: 12.6730 - acc: 0.2137\n",
      " 660/1036 [==================>...........] - ETA: 2:46:28 - loss: 12.6747 - acc: 0.2136\n",
      " 665/1036 [==================>...........] - ETA: 2:43:10 - loss: 12.6521 - acc: 0.2150\n",
      " 670/1036 [==================>...........] - ETA: 2:39:54 - loss: 12.6780 - acc: 0.2134\n",
      " 675/1036 [==================>...........] - ETA: 2:36:41 - loss: 12.6557 - acc: 0.2148\n",
      " 680/1036 [==================>...........] - ETA: 2:33:34 - loss: 12.6574 - acc: 0.2147\n",
      " 685/1036 [==================>...........] - ETA: 2:30:28 - loss: 12.6592 - acc: 0.2146\n",
      " 690/1036 [==================>...........] - ETA: 2:27:24 - loss: 12.6609 - acc: 0.2145\n",
      " 695/1036 [===================>..........] - ETA: 2:24:21 - loss: 12.6858 - acc: 0.2129\n",
      " 700/1036 [===================>..........] - ETA: 2:21:21 - loss: 12.6872 - acc: 0.2129\n",
      " 705/1036 [===================>..........] - ETA: 2:18:23 - loss: 12.6887 - acc: 0.2128\n",
      " 710/1036 [===================>..........] - ETA: 2:15:29 - loss: 12.6902 - acc: 0.2127\n",
      " 715/1036 [===================>..........] - ETA: 2:12:37 - loss: 12.7141 - acc: 0.2112\n",
      " 720/1036 [===================>..........] - ETA: 2:09:46 - loss: 12.7154 - acc: 0.2111\n",
      " 725/1036 [===================>..........] - ETA: 2:06:57 - loss: 12.7166 - acc: 0.2110\n",
      " 730/1036 [====================>.........] - ETA: 2:04:11 - loss: 12.7399 - acc: 0.2096\n",
      " 735/1036 [====================>.........] - ETA: 2:01:26 - loss: 12.7190 - acc: 0.2109\n",
      " 740/1036 [====================>.........] - ETA: 1:58:44 - loss: 12.6984 - acc: 0.2122\n",
      " 745/1036 [====================>.........] - ETA: 1:56:03 - loss: 12.6565 - acc: 0.2148\n",
      " 750/1036 [====================>.........] - ETA: 1:53:23 - loss: 12.6151 - acc: 0.2173\n",
      " 755/1036 [====================>.........] - ETA: 1:50:46 - loss: 12.6169 - acc: 0.2172\n",
      " 760/1036 [=====================>........] - ETA: 1:48:10 - loss: 12.5976 - acc: 0.2184\n",
      " 765/1036 [=====================>........] - ETA: 1:45:37 - loss: 12.5995 - acc: 0.2183\n",
      " 770/1036 [=====================>........] - ETA: 1:43:06 - loss: 12.5805 - acc: 0.2195\n",
      " 775/1036 [=====================>........] - ETA: 1:40:36 - loss: 12.5617 - acc: 0.2206\n",
      " 780/1036 [=====================>........] - ETA: 1:38:07 - loss: 12.5845 - acc: 0.2192\n",
      " 785/1036 [=====================>........] - ETA: 1:35:41 - loss: 12.5865 - acc: 0.2191\n",
      " 790/1036 [=====================>........] - ETA: 1:33:16 - loss: 12.5884 - acc: 0.2190\n",
      " 795/1036 [======================>.......] - ETA: 1:30:53 - loss: 12.6106 - acc: 0.2176\n",
      " 800/1036 [======================>.......] - ETA: 1:28:32 - loss: 12.6326 - acc: 0.2163\n",
      " 805/1036 [======================>.......] - ETA: 1:26:13 - loss: 12.6142 - acc: 0.2174\n",
      " 810/1036 [======================>.......] - ETA: 1:23:56 - loss: 12.6159 - acc: 0.2173\n",
      " 815/1036 [======================>.......] - ETA: 1:21:41 - loss: 12.6176 - acc: 0.2172\n",
      " 820/1036 [======================>.......] - ETA: 1:19:27 - loss: 12.5996 - acc: 0.2183\n",
      " 825/1036 [======================>.......] - ETA: 1:17:13 - loss: 12.5819 - acc: 0.2194\n",
      " 830/1036 [=======================>......] - ETA: 1:15:01 - loss: 12.5643 - acc: 0.2205\n",
      " 835/1036 [=======================>......] - ETA: 1:12:49 - loss: 12.5470 - acc: 0.2216"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc0a394b0bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# 実行。出力はなしで設定(verbose=0)。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m history = model.fit(X_train, y_train, batch_size=5, epochs=5,\n\u001b[0;32m---> 88\u001b[0;31m                    validation_data = (X_test, y_test), verbose = 1)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# plot_model(model, to_file=\"model.png\", show_shapes=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2333\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2334\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2336\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CNNを構成（VGG NET)\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import applications\n",
    "model = Sequential()\n",
    "\n",
    "initial_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=X_train.shape[1:])\n",
    "last = initial_model.output\n",
    "\n",
    "x = Flatten()(last)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "preds = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(initial_model.input, preds)\n",
    "# model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.summary()\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 実行。出力はなしで設定(verbose=0)。\n",
    "history = model.fit(X_train, y_train, batch_size=5, epochs=5,\n",
    "                   validation_data = (X_test, y_test), verbose = 1)\n",
    "\n",
    "# plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "# https://qiita.com/agumon/items/ab2de98a3783e0a93e66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-19のModelで層を組んでいる\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))       # クラスは2個\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 実行。出力はなしで設定(verbose=0)。\n",
    "history = model.fit(X_train, y_train, batch_size=5, epochs=5,\n",
    "                   validation_data = (X_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['acc', 'val_acc'], loc='lower right') #グラフの凡例を用意している\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに適用\n",
    "predict_classes = model.predict_classes(X_test)\n",
    "\n",
    "# マージ。yのデータは元に戻す\n",
    "mg_df = pd.DataFrame({'predict': predict_classes, 'class': np.argmax(y_test, axis=1)})\n",
    "\n",
    "# confusion matrix\n",
    "pd.crosstab(mg_df['class'], mg_df['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model,data,labels,data_test,labels_test):\n",
    "# def train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test]):\n",
    "    model.fit(data, labels, batch_size=32, epochs=30,verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "    return model.evaluate(data_test, labels_test, verbose=1)  # Evaluate the trained model on the test set!\n",
    "#損失値と評価の値を算出している\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "# print(cross_val_score(model, image_list, label_list,cv=kfold)).mean()  \n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "label = np.r_[np.repeat(0,16), np.repeat(1,0)]\n",
    "skf = StratifiedKFold(label, n_folds=15, shuffle=False)\n",
    "total_score = 0\n",
    "rep = 0\n",
    "print(\"success\")\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    total_score += train_and_evaluate_model(model, X_train[train], y_train[train], X_train[test], y_train[test])[1]\n",
    "    rep+=1\n",
    "# print(total_score/rep)\n",
    "print(\"%.10f\" % (total_score/rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 最初の3つのcellでとりあえず物体をopencvで検知するところまで行う\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import selectivesearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "\n",
    "import skimage.io\n",
    "import skimage.feature\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import skimage.util\n",
    "import skimage.segmentation\n",
    "import numpy\n",
    "\n",
    "\n",
    "# \"Selective Search for Object Recognition\" by J.R.R. Uijlings et al.\n",
    "#\n",
    "#  - Modified version with LBP extractor for texture vectorization\n",
    "\n",
    "\n",
    "def _generate_segments(im_orig, scale, sigma, min_size):\n",
    "    \"\"\"\n",
    "        segment smallest regions by the algorithm of Felzenswalb and\n",
    "        Huttenlocher\n",
    "    \"\"\"\n",
    "\n",
    "    # open the Image\n",
    "    im_mask = skimage.segmentation.felzenszwalb(\n",
    "        skimage.util.img_as_float(im_orig), scale=scale, sigma=sigma,\n",
    "        min_size=min_size)\n",
    "#     plt.imshow(im_mask)\n",
    "    # merge mask channel to the image as a 4th channel\n",
    "    im_orig = numpy.append(\n",
    "        im_orig, numpy.zeros(im_orig.shape[:2])[:, :, numpy.newaxis], axis=2)\n",
    "    im_orig[:, :, 3] = im_mask\n",
    "#     plt.imshow(im_orig)\n",
    "    return im_orig\n",
    "\n",
    "\n",
    "def _sim_colour(r1, r2):\n",
    "    \"\"\"\n",
    "        calculate the sum of histogram intersection of colour\n",
    "    \"\"\"\n",
    "    return sum([min(a, b) for a, b in zip(r1[\"hist_c\"], r2[\"hist_c\"])])\n",
    "\n",
    "\n",
    "def _sim_texture(r1, r2):\n",
    "    \"\"\"\n",
    "        calculate the sum of histogram intersection of texture\n",
    "    \"\"\"\n",
    "    return sum([min(a, b) for a, b in zip(r1[\"hist_t\"], r2[\"hist_t\"])])\n",
    "\n",
    "\n",
    "def _sim_size(r1, r2, imsize):\n",
    "    \"\"\"\n",
    "        calculate the size similarity over the image\n",
    "    \"\"\"\n",
    "    return 1.0 - (r1[\"size\"] + r2[\"size\"]) / imsize\n",
    "\n",
    "\n",
    "def _sim_fill(r1, r2, imsize):\n",
    "    \"\"\"\n",
    "        calculate the fill similarity over the image\n",
    "    \"\"\"\n",
    "    bbsize = (\n",
    "        (max(r1[\"max_x\"], r2[\"max_x\"]) - min(r1[\"min_x\"], r2[\"min_x\"]))\n",
    "        * (max(r1[\"max_y\"], r2[\"max_y\"]) - min(r1[\"min_y\"], r2[\"min_y\"]))\n",
    "    )\n",
    "    return 1.0 - (bbsize - r1[\"size\"] - r2[\"size\"]) / imsize\n",
    "\n",
    "\n",
    "def _calc_sim(r1, r2, imsize):\n",
    "    return (_sim_colour(r1, r2) + _sim_texture(r1, r2)\n",
    "            + _sim_size(r1, r2, imsize) + _sim_fill(r1, r2, imsize))\n",
    "\n",
    "\n",
    "def _calc_colour_hist(img):\n",
    "    \"\"\"\n",
    "        calculate colour histogram for each region\n",
    "\n",
    "        the size of output histogram will be BINS * COLOUR_CHANNELS(3)\n",
    "\n",
    "        number of bins is 25 as same as [uijlings_ijcv2013_draft.pdf]\n",
    "\n",
    "        extract HSV\n",
    "    \"\"\"\n",
    "\n",
    "    BINS = 25\n",
    "    hist = numpy.array([])\n",
    "\n",
    "    for colour_channel in (0, 1, 2):\n",
    "\n",
    "        # extracting one colour channel\n",
    "        c = img[:, colour_channel]\n",
    "\n",
    "        # calculate histogram for each colour and join to the result\n",
    "        hist = numpy.concatenate(\n",
    "            [hist] + [numpy.histogram(c, BINS, (0.0, 255.0))[0]])\n",
    "\n",
    "    # L1 normalize\n",
    "    hist = hist / len(img)\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def _calc_texture_gradient(img):\n",
    "    \"\"\"\n",
    "        calculate texture gradient for entire image\n",
    "\n",
    "        The original SelectiveSearch algorithm proposed Gaussian derivative\n",
    "        for 8 orientations, but we use LBP instead.\n",
    "\n",
    "        output will be [height(*)][width(*)]\n",
    "    \"\"\"\n",
    "    ret = numpy.zeros((img.shape[0], img.shape[1], img.shape[2]))\n",
    "\n",
    "    for colour_channel in (0, 1, 2):\n",
    "        ret[:, :, colour_channel] = skimage.feature.local_binary_pattern(\n",
    "            img[:, :, colour_channel], 8, 1.0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _calc_texture_hist(img):\n",
    "    \"\"\"\n",
    "        calculate texture histogram for each region\n",
    "\n",
    "        calculate the histogram of gradient for each colours\n",
    "        the size of output histogram will be\n",
    "            BINS * ORIENTATIONS * COLOUR_CHANNELS(3)\n",
    "    \"\"\"\n",
    "    BINS = 10\n",
    "\n",
    "    hist = numpy.array([])\n",
    "\n",
    "    for colour_channel in (0, 1, 2):\n",
    "\n",
    "        # mask by the colour channel\n",
    "        fd = img[:, colour_channel]\n",
    "\n",
    "        # calculate histogram for each orientation and concatenate them all\n",
    "        # and join to the result\n",
    "        hist = numpy.concatenate(\n",
    "            [hist] + [numpy.histogram(fd, BINS, (0.0, 1.0))[0]])\n",
    "\n",
    "    # L1 Normalize\n",
    "    hist = hist / len(img)\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def _extract_regions(img):\n",
    "\n",
    "    R = {}\n",
    "\n",
    "    # get hsv image\n",
    "    hsv = skimage.color.rgb2hsv(img[:, :, :3])\n",
    "\n",
    "    # pass 1: count pixel positions\n",
    "    for y, i in enumerate(img):\n",
    "\n",
    "        for x, (r, g, b, l) in enumerate(i):\n",
    "\n",
    "            # initialize a new region\n",
    "            if l not in R:\n",
    "                R[l] = {\n",
    "                    \"min_x\": 0xffff, \"min_y\": 0xffff,\n",
    "                    \"max_x\": 0, \"max_y\": 0, \"labels\": [l]}\n",
    "\n",
    "            # bounding box\n",
    "            if R[l][\"min_x\"] > x:\n",
    "                R[l][\"min_x\"] = x\n",
    "            if R[l][\"min_y\"] > y:\n",
    "                R[l][\"min_y\"] = y\n",
    "            if R[l][\"max_x\"] < x:\n",
    "                R[l][\"max_x\"] = x\n",
    "            if R[l][\"max_y\"] < y:\n",
    "                R[l][\"max_y\"] = y\n",
    "\n",
    "    # pass 2: calculate texture gradient\n",
    "    tex_grad = _calc_texture_gradient(img)\n",
    "\n",
    "    # pass 3: calculate colour histogram of each region\n",
    "    for k, v in list(R.items()):\n",
    "\n",
    "        # colour histogram\n",
    "        masked_pixels = hsv[:, :, :][img[:, :, 3] == k]\n",
    "        R[k][\"size\"] = len(masked_pixels / 4)\n",
    "        R[k][\"hist_c\"] = _calc_colour_hist(masked_pixels)\n",
    "\n",
    "        # texture histogram\n",
    "        R[k][\"hist_t\"] = _calc_texture_hist(tex_grad[:, :][img[:, :, 3] == k])\n",
    "#     print(R)\n",
    "    return R\n",
    "\n",
    "\n",
    "def _extract_neighbours(regions):\n",
    "\n",
    "    def intersect(a, b):\n",
    "        if (a[\"min_x\"] < b[\"min_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"min_y\"] < a[\"max_y\"]) or (\n",
    "            a[\"min_x\"] < b[\"max_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"max_y\"] < a[\"max_y\"]) or (\n",
    "            a[\"min_x\"] < b[\"min_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"max_y\"] < a[\"max_y\"]) or (\n",
    "            a[\"min_x\"] < b[\"max_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"min_y\"] < a[\"max_y\"]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    R = list(regions.items())\n",
    "    neighbours = []\n",
    "    for cur, a in enumerate(R[:-1]):\n",
    "        for b in R[cur + 1:]:\n",
    "            if intersect(a[1], b[1]):\n",
    "                neighbours.append((a, b))\n",
    "\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def _merge_regions(r1, r2):\n",
    "    new_size = r1[\"size\"] + r2[\"size\"]\n",
    "    rt = {\n",
    "        \"min_x\": min(r1[\"min_x\"], r2[\"min_x\"]),\n",
    "        \"min_y\": min(r1[\"min_y\"], r2[\"min_y\"]),\n",
    "        \"max_x\": max(r1[\"max_x\"], r2[\"max_x\"]),\n",
    "        \"max_y\": max(r1[\"max_y\"], r2[\"max_y\"]),\n",
    "        \"size\": new_size,\n",
    "        \"hist_c\": (\n",
    "            r1[\"hist_c\"] * r1[\"size\"] + r2[\"hist_c\"] * r2[\"size\"]) / new_size,\n",
    "        \"hist_t\": (\n",
    "            r1[\"hist_t\"] * r1[\"size\"] + r2[\"hist_t\"] * r2[\"size\"]) / new_size,\n",
    "        \"labels\": r1[\"labels\"] + r2[\"labels\"]\n",
    "    }\n",
    "    return rt\n",
    "\n",
    "\n",
    "def selective_search(\n",
    "        im_orig, scale=1.0, sigma=0.8, min_size=50):\n",
    "    '''Selective Search\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        im_orig : ndarray\n",
    "           写真\n",
    "        scale : int\n",
    "            Free parameter. Higher means larger clusters in felzenszwalb segmentation.\n",
    "        sigma : float\n",
    "            Width of Gaussian kernel for felzenszwalb segmentation.\n",
    "        min_size : int\n",
    "            Minimum component size for felzenszwalb segmentation.\n",
    "    Returns\n",
    "    -------\n",
    "        img : ndarray\n",
    "            image with region label\n",
    "            region label is stored in the 4th value of each pixel [r,g,b,(region)]\n",
    "        regions : array of dict\n",
    "            [\n",
    "                {\n",
    "                    'rect': (left, top, width, height),\n",
    "                    'labels': [...],\n",
    "                    'size': component_size\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "    '''\n",
    "    #3次元のデータでなければエラー文章を出力する\n",
    "    assert im_orig.shape[2] == 3, \"3ch image is expected\"\n",
    "\n",
    "    # load image and get smallest regions\n",
    "    # region label is stored in the 4th value of each pixel [r,g,b,(region)]\n",
    "    img = _generate_segments(im_orig, scale, sigma, min_size)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    if img is None:\n",
    "        return None, {}\n",
    "\n",
    "    imsize = img.shape[0] * img.shape[1]\n",
    "    R = _extract_regions(img)\n",
    "\n",
    "    # extract neighbouring information\n",
    "    neighbours = _extract_neighbours(R)\n",
    "\n",
    "    # calculate initial similarities\n",
    "    S = {}\n",
    "    for (ai, ar), (bi, br) in neighbours:\n",
    "        S[(ai, bi)] = _calc_sim(ar, br, imsize)\n",
    "\n",
    "    # hierarchal search\n",
    "    while S != {}:\n",
    "\n",
    "        # get highest similarity\n",
    "        i, j = sorted(S.items(), key=lambda i: i[1])[-1][0]\n",
    "\n",
    "        # merge corresponding regions\n",
    "        t = max(R.keys()) + 1.0\n",
    "        R[t] = _merge_regions(R[i], R[j])\n",
    "\n",
    "        # mark similarities for regions to be removed\n",
    "        key_to_delete = []\n",
    "        for k, v in list(S.items()):\n",
    "            if (i in k) or (j in k):\n",
    "                key_to_delete.append(k)\n",
    "\n",
    "        # remove old similarities of related regions\n",
    "        for k in key_to_delete:\n",
    "            del S[k]\n",
    "\n",
    "        # calculate similarity set with the new region\n",
    "        for k in [a for a in key_to_delete if a != (i, j)]:\n",
    "            n = k[1] if k[0] in (i, j) else k[0]\n",
    "            S[(t, n)] = _calc_sim(R[t], R[n], imsize)\n",
    "\n",
    "    regions = []\n",
    "    for k, r in list(R.items()):\n",
    "        regions.append({\n",
    "            'rect': (\n",
    "                r['min_x'], r['min_y'],\n",
    "                r['max_x'] - r['min_x'], r['max_y'] - r['min_y']),\n",
    "            'size': r['size'],\n",
    "            'labels': r['labels']\n",
    "        })\n",
    "\n",
    "    return img, regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "        # loading lena image\n",
    "    img = cv2.imread(\"rice.png\")\n",
    "\n",
    "    # perform selective search\n",
    "    img_lbl, regions = selective_search(\n",
    "        img, scale=500, sigma=0.9, min_size=10)\n",
    "#     print(regions)\n",
    "#     img.show()\n",
    "    candidates = set()\n",
    "    for r in regions:\n",
    "        # excluding same rectangle (with different segments)\n",
    "        if r['rect'] in candidates:\n",
    "            continue\n",
    "        # excluding regions smaller than 2000 pixels\n",
    "        if r['size'] < 2000:\n",
    "            continue\n",
    "        # distorted rects\n",
    "        x, y, w, h = r['rect']\n",
    "#         if w / h > 0.5 or h / w > 0.5:\n",
    "        if w / h > 1.2 or h / w > 1.2:\n",
    "            continue\n",
    "        candidates.add(r['rect'])\n",
    "\n",
    "    # draw rectangles on the original image\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))\n",
    "    ax.imshow(img)\n",
    "    X = []\n",
    "    Y = []\n",
    "#     print(candidates)\n",
    "    for x, y, w, h in candidates:\n",
    "#         print(x, y, w, h)\n",
    "        rect = mpatches.Rectangle(\n",
    "            (x, y), w, h, fill=False, edgecolor='red', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        X.append(img_to_array(imresize(img[y:y + h, x:x + w],(64,64), interp='nearest')))\n",
    "        # cv2.imwrite('' + str(x) + '.jpg', img[y:y + h, x:x + w])\n",
    "    X = np.asarray(X)\n",
    "#     print(X.shape)\n",
    "    predictions = model.predict(X)\n",
    "        # round predictions\n",
    "#     rounded = [round(X[0]) for x in predictions]\n",
    "    print(predictions)\n",
    "    for prediction in predictions:\n",
    "        prediction = list(prediction)\n",
    "        print(name_class[prediction.index(max(prediction))])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-TensorFlow",
   "language": "python",
   "name": "py35_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
