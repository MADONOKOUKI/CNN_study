{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.topology.Layer'>\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, list_pictures, load_img\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.gridspec as gridspec\n",
    "import shutil\n",
    "from keras.utils import plot_model\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from scipy.misc import imresize\n",
    "from skimage import io\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-db8b4e782e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         print(picture)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#     img =load_img(picture, target_size=(64,64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m#         img =imresize(skimage.io.imread(picture),(64,64), interp='nearest')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#     img_raw = skimage.io.imread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_grey, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                                (plugin, kind))\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mpil_to_ndarray\u001b[0;34m(im, dtype, img_num)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# this will raise an IOError if the file is not readable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://pillow.readthedocs.org/en/latest/installation.html#external-libraries\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mgetdata\u001b[0;34m(self, band)\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \"\"\"\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mband\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetband\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# フォルダの中にある画像を順次読み込む\n",
    "# カテゴリーは0から始める\n",
    "\n",
    "#参考\n",
    "#https://qiita.com/supersaiakujin/items/fc54116df9ca6958a68d\n",
    "#http://testpy.hatenablog.com/entry/2017/06/02/001901\n",
    "#https://qiita.com/zaburo/items/0b9db87d0a52191b164b\n",
    "# 画像の反転と回転の実装、またはdata argumentation の実装を行う\n",
    "X = []\n",
    "Y = []\n",
    "name_class = []\n",
    "#下、後でコード量を減らす\n",
    "# 対象Aの画像\n",
    "rate = 0.2\n",
    "label = 0\n",
    "for dir in os.listdir(\"data/train\"):\n",
    "#     print(dir)\n",
    "    if dir == \".DS_Store\":\n",
    "        continue\n",
    "    name_class.append(dir)\n",
    "    for picture in list_pictures(\"data/train/\"+dir):\n",
    "#         print(picture)\n",
    "    #     img =load_img(picture, target_size=(64,64))\n",
    "        img =imresize(skimage.io.imread(picture),(224,224), interp='nearest')\n",
    "#         img =imresize(skimage.io.imread(picture),(64,64), interp='nearest')\n",
    "    #     img_raw = skimage.io.imread\n",
    "#         print(img_to_array(skimage.transform.rotate(img, angle=0, resize=False, center=None)).shape)\n",
    "        for i in range(358,360):\n",
    "            #回転のみ\n",
    "            X.append(img_to_array(skimage.transform.rotate(img, angle=i, resize=False, center=None)))\n",
    "            Y.append(label )\n",
    "#             #回転と上下の反転\n",
    "            X.append(img_to_array(cv2.flip(skimage.transform.rotate(img, angle=i, resize=False, center=None),0)))\n",
    "            Y.append(label )\n",
    "#             #回転と左右の反転\n",
    "#             X.append(img_to_array(cv2.flip(skimage.transform.rotate(img, angle=i, resize=False, center=None),1)))\n",
    "#             Y.append(label )\n",
    "#             #拡大\n",
    "            X.append(img_to_array(skimage.transform.warp(skimage.transform.rotate(img, angle=i, resize=False, center=None), skimage.transform.AffineTransform(scale=(1-rate, 1-rate), translation=(64,64)))))\n",
    "            Y.append(label )\n",
    "    label += 1\n",
    "\n",
    "# # arrayに変換\n",
    "# X.shape()\n",
    "print(name_class)\n",
    "X = np.asarray(X)\n",
    "print(X.shape)\n",
    "# print(X)\n",
    "Y = np.asarray(Y)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画素値を0から1の範囲に変換\n",
    "X = X.astype('float32')\n",
    "X = X / 255.0\n",
    "\n",
    "# クラスの形式を変換\n",
    "Y = np_utils.to_categorical(Y, 5)\n",
    "\n",
    "# 学習用データとテストデータ\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNNを構築(kaggleコンペで高かった組み方)\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                  input_shape=X_train.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Conv2D(32, (3, 3)))\n",
    "# # model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# # model.add(Activation('relu'))\n",
    "# # model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(5))       # クラスは2個\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# # コンパイル\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='SGD',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # 実行。出力はなしで設定(verbose=0)。\n",
    "# history = model.fit(X_train, y_train, batch_size=5, epochs=50,\n",
    "#                    validation_data = (X_test, y_test), verbose = 1)\n",
    "\n",
    "# # plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "# # https://qiita.com/agumon/items/ab2de98a3783e0a93e66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNを構成（VGG NET)\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import applications\n",
    "model = Sequential()\n",
    "\n",
    "initial_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=X_train.shape[1:])\n",
    "last = initial_model.output\n",
    "\n",
    "x = Flatten()(last)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "preds = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(initial_model.input, preds)\n",
    "# model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.summary()\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 実行。出力はなしで設定(verbose=0)。\n",
    "history = model.fit(X_train, y_train, batch_size=5, epochs=5,\n",
    "                   validation_data = (X_test, y_test), verbose = 1)\n",
    "\n",
    "# plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "# https://qiita.com/agumon/items/ab2de98a3783e0a93e66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-19のModelで層を組んでいる\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(512,(3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))       # クラスは2個\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 実行。出力はなしで設定(verbose=0)。\n",
    "history = model.fit(X_train, y_train, batch_size=5, epochs=5,\n",
    "                   validation_data = (X_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['acc', 'val_acc'], loc='lower right') #グラフの凡例を用意している\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに適用\n",
    "predict_classes = model.predict_classes(X_test)\n",
    "\n",
    "# マージ。yのデータは元に戻す\n",
    "mg_df = pd.DataFrame({'predict': predict_classes, 'class': np.argmax(y_test, axis=1)})\n",
    "\n",
    "# confusion matrix\n",
    "pd.crosstab(mg_df['class'], mg_df['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model,data,labels,data_test,labels_test):\n",
    "# def train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test]):\n",
    "    model.fit(data, labels, batch_size=32, epochs=30,verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "    return model.evaluate(data_test, labels_test, verbose=1)  # Evaluate the trained model on the test set!\n",
    "#損失値と評価の値を算出している\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "# print(cross_val_score(model, image_list, label_list,cv=kfold)).mean()  \n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "label = np.r_[np.repeat(0,16), np.repeat(1,0)]\n",
    "skf = StratifiedKFold(label, n_folds=15, shuffle=False)\n",
    "total_score = 0\n",
    "rep = 0\n",
    "print(\"success\")\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    total_score += train_and_evaluate_model(model, X_train[train], y_train[train], X_train[test], y_train[test])[1]\n",
    "    rep+=1\n",
    "# print(total_score/rep)\n",
    "print(\"%.10f\" % (total_score/rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 最初の3つのcellでとりあえず物体をopencvで検知するところまで行う\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import selectivesearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "\n",
    "import skimage.io\n",
    "import skimage.feature\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import skimage.util\n",
    "import skimage.segmentation\n",
    "import numpy\n",
    "\n",
    "\n",
    "# \"Selective Search for Object Recognition\" by J.R.R. Uijlings et al.\n",
    "#\n",
    "#  - Modified version with LBP extractor for texture vectorization\n",
    "\n",
    "\n",
    "def _generate_segments(im_orig, scale, sigma, min_size):\n",
    "    \"\"\"\n",
    "        segment smallest regions by the algorithm of Felzenswalb and\n",
    "        Huttenlocher\n",
    "    \"\"\"\n",
    "\n",
    "    # open the Image\n",
    "    im_mask = skimage.segmentation.felzenszwalb(\n",
    "        skimage.util.img_as_float(im_orig), scale=scale, sigma=sigma,\n",
    "        min_size=min_size)\n",
    "#     plt.imshow(im_mask)\n",
    "    # merge mask channel to the image as a 4th channel\n",
    "    im_orig = numpy.append(\n",
    "        im_orig, numpy.zeros(im_orig.shape[:2])[:, :, numpy.newaxis], axis=2)\n",
    "    im_orig[:, :, 3] = im_mask\n",
    "#     plt.imshow(im_orig)\n",
    "    return im_orig\n",
    "\n",
    "\n",
    "def _sim_colour(r1, r2):\n",
    "    \"\"\"\n",
    "        calculate the sum of histogram intersection of colour\n",
    "    \"\"\"\n",
    "    return sum([min(a, b) for a, b in zip(r1[\"hist_c\"], r2[\"hist_c\"])])\n",
    "\n",
    "\n",
    "def _sim_texture(r1, r2):\n",
    "    \"\"\"\n",
    "        calculate the sum of histogram intersection of texture\n",
    "    \"\"\"\n",
    "    return sum([min(a, b) for a, b in zip(r1[\"hist_t\"], r2[\"hist_t\"])])\n",
    "\n",
    "\n",
    "def _sim_size(r1, r2, imsize):\n",
    "    \"\"\"\n",
    "        calculate the size similarity over the image\n",
    "    \"\"\"\n",
    "    return 1.0 - (r1[\"size\"] + r2[\"size\"]) / imsize\n",
    "\n",
    "\n",
    "def _sim_fill(r1, r2, imsize):\n",
    "    \"\"\"\n",
    "        calculate the fill similarity over the image\n",
    "    \"\"\"\n",
    "    bbsize = (\n",
    "        (max(r1[\"max_x\"], r2[\"max_x\"]) - min(r1[\"min_x\"], r2[\"min_x\"]))\n",
    "        * (max(r1[\"max_y\"], r2[\"max_y\"]) - min(r1[\"min_y\"], r2[\"min_y\"]))\n",
    "    )\n",
    "    return 1.0 - (bbsize - r1[\"size\"] - r2[\"size\"]) / imsize\n",
    "\n",
    "\n",
    "def _calc_sim(r1, r2, imsize):\n",
    "    return (_sim_colour(r1, r2) + _sim_texture(r1, r2)\n",
    "            + _sim_size(r1, r2, imsize) + _sim_fill(r1, r2, imsize))\n",
    "\n",
    "\n",
    "def _calc_colour_hist(img):\n",
    "    \"\"\"\n",
    "        calculate colour histogram for each region\n",
    "\n",
    "        the size of output histogram will be BINS * COLOUR_CHANNELS(3)\n",
    "\n",
    "        number of bins is 25 as same as [uijlings_ijcv2013_draft.pdf]\n",
    "\n",
    "        extract HSV\n",
    "    \"\"\"\n",
    "\n",
    "    BINS = 25\n",
    "    hist = numpy.array([])\n",
    "\n",
    "    for colour_channel in (0, 1, 2):\n",
    "\n",
    "        # extracting one colour channel\n",
    "        c = img[:, colour_channel]\n",
    "\n",
    "        # calculate histogram for each colour and join to the result\n",
    "        hist = numpy.concatenate(\n",
    "            [hist] + [numpy.histogram(c, BINS, (0.0, 255.0))[0]])\n",
    "\n",
    "    # L1 normalize\n",
    "    hist = hist / len(img)\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def _calc_texture_gradient(img):\n",
    "    \"\"\"\n",
    "        calculate texture gradient for entire image\n",
    "\n",
    "        The original SelectiveSearch algorithm proposed Gaussian derivative\n",
    "        for 8 orientations, but we use LBP instead.\n",
    "\n",
    "        output will be [height(*)][width(*)]\n",
    "    \"\"\"\n",
    "    ret = numpy.zeros((img.shape[0], img.shape[1], img.shape[2]))\n",
    "\n",
    "    for colour_channel in (0, 1, 2):\n",
    "        ret[:, :, colour_channel] = skimage.feature.local_binary_pattern(\n",
    "            img[:, :, colour_channel], 8, 1.0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _calc_texture_hist(img):\n",
    "    \"\"\"\n",
    "        calculate texture histogram for each region\n",
    "\n",
    "        calculate the histogram of gradient for each colours\n",
    "        the size of output histogram will be\n",
    "            BINS * ORIENTATIONS * COLOUR_CHANNELS(3)\n",
    "    \"\"\"\n",
    "    BINS = 10\n",
    "\n",
    "    hist = numpy.array([])\n",
    "\n",
    "    for colour_channel in (0, 1, 2):\n",
    "\n",
    "        # mask by the colour channel\n",
    "        fd = img[:, colour_channel]\n",
    "\n",
    "        # calculate histogram for each orientation and concatenate them all\n",
    "        # and join to the result\n",
    "        hist = numpy.concatenate(\n",
    "            [hist] + [numpy.histogram(fd, BINS, (0.0, 1.0))[0]])\n",
    "\n",
    "    # L1 Normalize\n",
    "    hist = hist / len(img)\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def _extract_regions(img):\n",
    "\n",
    "    R = {}\n",
    "\n",
    "    # get hsv image\n",
    "    hsv = skimage.color.rgb2hsv(img[:, :, :3])\n",
    "\n",
    "    # pass 1: count pixel positions\n",
    "    for y, i in enumerate(img):\n",
    "\n",
    "        for x, (r, g, b, l) in enumerate(i):\n",
    "\n",
    "            # initialize a new region\n",
    "            if l not in R:\n",
    "                R[l] = {\n",
    "                    \"min_x\": 0xffff, \"min_y\": 0xffff,\n",
    "                    \"max_x\": 0, \"max_y\": 0, \"labels\": [l]}\n",
    "\n",
    "            # bounding box\n",
    "            if R[l][\"min_x\"] > x:\n",
    "                R[l][\"min_x\"] = x\n",
    "            if R[l][\"min_y\"] > y:\n",
    "                R[l][\"min_y\"] = y\n",
    "            if R[l][\"max_x\"] < x:\n",
    "                R[l][\"max_x\"] = x\n",
    "            if R[l][\"max_y\"] < y:\n",
    "                R[l][\"max_y\"] = y\n",
    "\n",
    "    # pass 2: calculate texture gradient\n",
    "    tex_grad = _calc_texture_gradient(img)\n",
    "\n",
    "    # pass 3: calculate colour histogram of each region\n",
    "    for k, v in list(R.items()):\n",
    "\n",
    "        # colour histogram\n",
    "        masked_pixels = hsv[:, :, :][img[:, :, 3] == k]\n",
    "        R[k][\"size\"] = len(masked_pixels / 4)\n",
    "        R[k][\"hist_c\"] = _calc_colour_hist(masked_pixels)\n",
    "\n",
    "        # texture histogram\n",
    "        R[k][\"hist_t\"] = _calc_texture_hist(tex_grad[:, :][img[:, :, 3] == k])\n",
    "#     print(R)\n",
    "    return R\n",
    "\n",
    "\n",
    "def _extract_neighbours(regions):\n",
    "\n",
    "    def intersect(a, b):\n",
    "        if (a[\"min_x\"] < b[\"min_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"min_y\"] < a[\"max_y\"]) or (\n",
    "            a[\"min_x\"] < b[\"max_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"max_y\"] < a[\"max_y\"]) or (\n",
    "            a[\"min_x\"] < b[\"min_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"max_y\"] < a[\"max_y\"]) or (\n",
    "            a[\"min_x\"] < b[\"max_x\"] < a[\"max_x\"]\n",
    "                and a[\"min_y\"] < b[\"min_y\"] < a[\"max_y\"]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    R = list(regions.items())\n",
    "    neighbours = []\n",
    "    for cur, a in enumerate(R[:-1]):\n",
    "        for b in R[cur + 1:]:\n",
    "            if intersect(a[1], b[1]):\n",
    "                neighbours.append((a, b))\n",
    "\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def _merge_regions(r1, r2):\n",
    "    new_size = r1[\"size\"] + r2[\"size\"]\n",
    "    rt = {\n",
    "        \"min_x\": min(r1[\"min_x\"], r2[\"min_x\"]),\n",
    "        \"min_y\": min(r1[\"min_y\"], r2[\"min_y\"]),\n",
    "        \"max_x\": max(r1[\"max_x\"], r2[\"max_x\"]),\n",
    "        \"max_y\": max(r1[\"max_y\"], r2[\"max_y\"]),\n",
    "        \"size\": new_size,\n",
    "        \"hist_c\": (\n",
    "            r1[\"hist_c\"] * r1[\"size\"] + r2[\"hist_c\"] * r2[\"size\"]) / new_size,\n",
    "        \"hist_t\": (\n",
    "            r1[\"hist_t\"] * r1[\"size\"] + r2[\"hist_t\"] * r2[\"size\"]) / new_size,\n",
    "        \"labels\": r1[\"labels\"] + r2[\"labels\"]\n",
    "    }\n",
    "    return rt\n",
    "\n",
    "\n",
    "def selective_search(\n",
    "        im_orig, scale=1.0, sigma=0.8, min_size=50):\n",
    "    '''Selective Search\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        im_orig : ndarray\n",
    "           写真\n",
    "        scale : int\n",
    "            Free parameter. Higher means larger clusters in felzenszwalb segmentation.\n",
    "        sigma : float\n",
    "            Width of Gaussian kernel for felzenszwalb segmentation.\n",
    "        min_size : int\n",
    "            Minimum component size for felzenszwalb segmentation.\n",
    "    Returns\n",
    "    -------\n",
    "        img : ndarray\n",
    "            image with region label\n",
    "            region label is stored in the 4th value of each pixel [r,g,b,(region)]\n",
    "        regions : array of dict\n",
    "            [\n",
    "                {\n",
    "                    'rect': (left, top, width, height),\n",
    "                    'labels': [...],\n",
    "                    'size': component_size\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "    '''\n",
    "    #3次元のデータでなければエラー文章を出力する\n",
    "    assert im_orig.shape[2] == 3, \"3ch image is expected\"\n",
    "\n",
    "    # load image and get smallest regions\n",
    "    # region label is stored in the 4th value of each pixel [r,g,b,(region)]\n",
    "    img = _generate_segments(im_orig, scale, sigma, min_size)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    if img is None:\n",
    "        return None, {}\n",
    "\n",
    "    imsize = img.shape[0] * img.shape[1]\n",
    "    R = _extract_regions(img)\n",
    "\n",
    "    # extract neighbouring information\n",
    "    neighbours = _extract_neighbours(R)\n",
    "\n",
    "    # calculate initial similarities\n",
    "    S = {}\n",
    "    for (ai, ar), (bi, br) in neighbours:\n",
    "        S[(ai, bi)] = _calc_sim(ar, br, imsize)\n",
    "\n",
    "    # hierarchal search\n",
    "    while S != {}:\n",
    "\n",
    "        # get highest similarity\n",
    "        i, j = sorted(S.items(), key=lambda i: i[1])[-1][0]\n",
    "\n",
    "        # merge corresponding regions\n",
    "        t = max(R.keys()) + 1.0\n",
    "        R[t] = _merge_regions(R[i], R[j])\n",
    "\n",
    "        # mark similarities for regions to be removed\n",
    "        key_to_delete = []\n",
    "        for k, v in list(S.items()):\n",
    "            if (i in k) or (j in k):\n",
    "                key_to_delete.append(k)\n",
    "\n",
    "        # remove old similarities of related regions\n",
    "        for k in key_to_delete:\n",
    "            del S[k]\n",
    "\n",
    "        # calculate similarity set with the new region\n",
    "        for k in [a for a in key_to_delete if a != (i, j)]:\n",
    "            n = k[1] if k[0] in (i, j) else k[0]\n",
    "            S[(t, n)] = _calc_sim(R[t], R[n], imsize)\n",
    "\n",
    "    regions = []\n",
    "    for k, r in list(R.items()):\n",
    "        regions.append({\n",
    "            'rect': (\n",
    "                r['min_x'], r['min_y'],\n",
    "                r['max_x'] - r['min_x'], r['max_y'] - r['min_y']),\n",
    "            'size': r['size'],\n",
    "            'labels': r['labels']\n",
    "        })\n",
    "\n",
    "    return img, regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "        # loading lena image\n",
    "    img = cv2.imread(\"rice.png\")\n",
    "\n",
    "    # perform selective search\n",
    "    img_lbl, regions = selective_search(\n",
    "        img, scale=500, sigma=0.9, min_size=10)\n",
    "#     print(regions)\n",
    "#     img.show()\n",
    "    candidates = set()\n",
    "    for r in regions:\n",
    "        # excluding same rectangle (with different segments)\n",
    "        if r['rect'] in candidates:\n",
    "            continue\n",
    "        # excluding regions smaller than 2000 pixels\n",
    "        if r['size'] < 2000:\n",
    "            continue\n",
    "        # distorted rects\n",
    "        x, y, w, h = r['rect']\n",
    "#         if w / h > 0.5 or h / w > 0.5:\n",
    "        if w / h > 1.2 or h / w > 1.2:\n",
    "            continue\n",
    "        candidates.add(r['rect'])\n",
    "\n",
    "    # draw rectangles on the original image\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))\n",
    "    ax.imshow(img)\n",
    "    X = []\n",
    "    Y = []\n",
    "#     print(candidates)\n",
    "    for x, y, w, h in candidates:\n",
    "#         print(x, y, w, h)\n",
    "        rect = mpatches.Rectangle(\n",
    "            (x, y), w, h, fill=False, edgecolor='red', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        X.append(img_to_array(imresize(img[y:y + h, x:x + w],(64,64), interp='nearest')))\n",
    "        # cv2.imwrite('' + str(x) + '.jpg', img[y:y + h, x:x + w])\n",
    "    X = np.asarray(X)\n",
    "#     print(X.shape)\n",
    "    predictions = model.predict(X)\n",
    "        # round predictions\n",
    "#     rounded = [round(X[0]) for x in predictions]\n",
    "    print(predictions)\n",
    "    for prediction in predictions:\n",
    "        prediction = list(prediction)\n",
    "        print(name_class[prediction.index(max(prediction))])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-TensorFlow",
   "language": "python",
   "name": "py35_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
