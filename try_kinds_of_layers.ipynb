{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "{\"username\":\"\",\"version\":\"5.2\",\"session\":\"b7463fd38072216bd6aa3cb87148f5e3\",\"msg_id\":\"f0ade89abd40530ec6b46f0c99d18a44\",\"msg_type\":\"execute_request\",\"date\":\"2017-11-10T08:19:24.977843Z\"}\n",
      "2\n",
      "{}\n",
      "2\n",
      "{}\n",
      "359\n",
      "{\"silent\":false,\"store_history\":true,\"user_expressions\":{},\"allow_stdin\":true,\"stop_on_error\":true,\"code\":\"from keras.models import Sequential\\nfrom keras.layers import Activation,Dense,Dropout\\nfrom np_utils import to_categorical\\nfrom keras.optimizers import Adagrad\\nfrom keras.optimizers import Adam\\nimport numpy as np\\nfrom PIL import Image\\nimport os\"}\n",
      "121\n",
      "{\n",
      "    \"epsilon\": 1e-07,\n",
      "    \"floatx\": \"float32\",\n",
      "    \"image_data_format\": \"channels_last\",\n",
      "    \"backend\": \"tensorflow\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Dropout\n",
    "from np_utils import to_categorical\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "{\"username\":\"\",\"version\":\"5.2\",\"session\":\"b7463fd38072216bd6aa3cb87148f5e3\",\"msg_id\":\"828c41fa361ed72c9f7047b358b85e61\",\"msg_type\":\"execute_request\",\"date\":\"2017-11-10T08:19:29.149351Z\"}\n",
      "2\n",
      "{}\n",
      "2\n",
      "{}\n",
      "702\n",
      "{\"silent\":false,\"store_history\":true,\"user_expressions\":{},\"allow_stdin\":true,\"stop_on_error\":true,\"code\":\"from keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Flatten\\nfrom keras.layers import Conv2D, MaxPooling2D\\ndef multi_layer_perceptron():\\n    model = Sequential()\\n    model.add(Conv2D(32, kernel_size=(3, 3),\\n                     activation='relu',\\n                     input_shape=1875))\\n    model.add(Conv2D(64, (3, 3), activation='relu'))\\n    model.add(MaxPooling2D(pool_size=(2, 2)))\\n    model.add(Dropout(0.25))\\n    model.add(Flatten())\\n    model.add(Dense(128, activation='relu'))\\n    model.add(Dropout(0.5))\\n    model.add(Dense(Activation='softmax'))\"}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "def multi_layer_perceptron():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=1875))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(Activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "{\"username\":\"\",\"version\":\"5.2\",\"session\":\"b7463fd38072216bd6aa3cb87148f5e3\",\"msg_id\":\"00da5da4a99c67469a6ff467826ea466\",\"msg_type\":\"execute_request\",\"date\":\"2017-11-10T08:13:06.561098Z\"}\n",
      "2\n",
      "{}\n",
      "2\n",
      "{}\n",
      "555\n",
      "{\"silent\":false,\"store_history\":true,\"user_expressions\":{},\"allow_stdin\":true,\"stop_on_error\":true,\"code\":\"# learning data\\ndef multi_layer_perceptron():\\n  #モデルを生成してニューラルネットを構築\\n  model = Sequential()\\n  model.add(Dense(200,input_dim=1875)) # 入力層1875ノード, 隠れ層に200ノード, 全結合\\n  model.add(Activation(\\\"relu\\\")) #活性化関数relu\\n  model.add(Dropout(0.2))\\n\\n  model.add(Dense(200)) #中間層200ノード\\n  model.add(Activation(\\\"relu\\\")) #活性化関数relu\\n  model.add(Dropout(0.2))\\n\\n  model.add(Dense(2)) #出力層3ノード,全結合している\\n  model.add(Activation(\\\"softmax\\\"))\\n\\n  return model\"}\n"
     ]
    }
   ],
   "source": [
    "# learning data\n",
    "def multi_layer_perceptron():\n",
    "  #モデルを生成してニューラルネットを構築\n",
    "  model = Sequential()\n",
    "  model.add(Dense(200,input_dim=1875)) # 入力層1875ノード, 隠れ層に200ノード, 全結合\n",
    "  model.add(Activation(\"relu\")) #活性化関数relu\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Dense(200)) #中間層200ノード\n",
    "  model.add(Activation(\"relu\")) #活性化関数relu\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Dense(2)) #出力層3ノード,全結合している\n",
    "  model.add(Activation(\"softmax\"))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "{\"username\":\"\",\"version\":\"5.2\",\"session\":\"b7463fd38072216bd6aa3cb87148f5e3\",\"msg_id\":\"491b5e597779e3c822ef6c46a1dff189\",\"msg_type\":\"execute_request\",\"date\":\"2017-11-10T08:18:38.952322Z\"}\n",
      "2\n",
      "{}\n",
      "2\n",
      "{}\n",
      "1666\n",
      "{\"silent\":false,\"store_history\":true,\"user_expressions\":{},\"allow_stdin\":true,\"stop_on_error\":true,\"code\":\"image_list = []\\nlabel_list = []\\n\\nfor dir in os.listdir(\\\"data/train\\\"):\\n  if dir == \\\".DS_Store\\\":\\n    continue\\n\\n  dir1 = \\\"data/train/\\\" + dir\\n  label = 0\\n\\n  if dir == \\\"orion\\\": #onionのlabel = 0\\n    label = 0\\n  elif dir == \\\"carot\\\": #carotのlabel = 0\\n    label = 1\\n  else:\\n    continue\\n  for file in os.listdir(dir1):\\n    if file != \\\".DS_Store\\\":\\n      # 配列label_listに正解ラベルを追加(りんご:0 オレンジ:1)\\n      label_list.append(label)\\n      filepath = dir1 + \\\"/\\\" + file\\n      # 画像を25x25pixelに変換し、1要素が[R,G,B]3要素を含む配列の25x25の２次元配列として読み込む。\\n      # [R,G,B]はそれぞれが0-255の配列。\\n      image = np.array(Image.open(filepath).resize((25,25)))\\n      print(filepath)\\n      # 配列を変換し、[[Redの配列],[Greenの配列],[Blueの配列]] のような形にする。\\n      print(image.shape)\\n      image = image.transpose(2,0,1)\\n      #さらにフラットな1次元配列に変換、最初の1/3がred,次がgreen、最後がblueの要素がフラットに並ぶ\\n      print(image.shape)\\n      image = image.reshape(1,image.shape[0] * image.shape[1] * image.shape[2]).astype(\\\"float32\\\")[0]\\n      #配列をimage_listに保存する\\n      print(image.shape)\\n      image_list.append(image/255.)\\n\\n# kerasに渡すためにnumpy配列に変換\\nimage_list = np.array(image_list)\\nprint(image_list.shape)\\n# 0 -> [1,0], 1 -> [0,1] という感じ。\\n#ラベルの配列を1と0からなるラベル配列に変換する\\nY = to_categorical(label_list)\\nprint(Y.shape)\\nmodel = multi_layer_perceptron()\\n#optimizer → Adamを使用\\nopt = Adam(lr=0.001)\\n#compiling model\\nmodel.compile(loss=\\\"categorical_crossentropy\\\",optimizer=opt,metrics=[\\\"accuracy\\\"])\\n#学習の実行\\n# epochs 繰り返し学習させる回数\\nmodel.fit(image_list,Y,nb_epoch=1500,batch_size = 100,validation_split=0.1)\\n\"}\n",
      "data/train/carot/0.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/1.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/10.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/11.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/12.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/13.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/14.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/15.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/16.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/17.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/18.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/19.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/2.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/20.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/21.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/23.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/24.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/25.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/26.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/27.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/28.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/29.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/3.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/31.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/32.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/33.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/34.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/35.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/36.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/37.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/38.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/39.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/4.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/40.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/41.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/42.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/43.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/44.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/45.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/46.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/47.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/48.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/49.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/5.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/51.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/52.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/53.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/54.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/56.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/57.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/58.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/59.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/6.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/7.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/8.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "data/train/carot/9.jpg\n",
      "(25, 25, 3)\n",
      "(3, 25, 25)\n",
      "(1875,)\n",
      "(56, 1875)\n",
      "(56, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d7f9a1bd3092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_layer_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#optimizer → Adamを使用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2b80b9a483a3>\u001b[0m in \u001b[0;36mmulti_layer_perceptron\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     model.add(Conv2D(32, kernel_size=(3, 3),\n\u001b[1;32m      7\u001b[0m                      \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                      input_shape=1875))\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m                  \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                  **kwargs):\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kokimadono/grade3/proB/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "image_list = []\n",
    "label_list = []\n",
    "\n",
    "for dir in os.listdir(\"data/train\"):\n",
    "  if dir == \".DS_Store\":\n",
    "    continue\n",
    "\n",
    "  dir1 = \"data/train/\" + dir\n",
    "  label = 0\n",
    "\n",
    "  if dir == \"orion\": #onionのlabel = 0\n",
    "    label = 0\n",
    "  elif dir == \"carot\": #carotのlabel = 0\n",
    "    label = 1\n",
    "  else:\n",
    "    continue\n",
    "  for file in os.listdir(dir1):\n",
    "    if file != \".DS_Store\":\n",
    "      # 配列label_listに正解ラベルを追加(りんご:0 オレンジ:1)\n",
    "      label_list.append(label)\n",
    "      filepath = dir1 + \"/\" + file\n",
    "      # 画像を25x25pixelに変換し、1要素が[R,G,B]3要素を含む配列の25x25の２次元配列として読み込む。\n",
    "      # [R,G,B]はそれぞれが0-255の配列。\n",
    "      image = np.array(Image.open(filepath).resize((25,25)))\n",
    "      print(filepath)\n",
    "      # 配列を変換し、[[Redの配列],[Greenの配列],[Blueの配列]] のような形にする。\n",
    "      print(image.shape)\n",
    "      image = image.transpose(2,0,1)\n",
    "      #さらにフラットな1次元配列に変換、最初の1/3がred,次がgreen、最後がblueの要素がフラットに並ぶ\n",
    "      print(image.shape)\n",
    "      image = image.reshape(1,image.shape[0] * image.shape[1] * image.shape[2]).astype(\"float32\")[0]\n",
    "      #配列をimage_listに保存する\n",
    "      print(image.shape)\n",
    "      image_list.append(image/255.)\n",
    "\n",
    "# kerasに渡すためにnumpy配列に変換\n",
    "image_list = np.array(image_list)\n",
    "print(image_list.shape)\n",
    "# 0 -> [1,0], 1 -> [0,1] という感じ。\n",
    "#ラベルの配列を1と0からなるラベル配列に変換する\n",
    "Y = to_categorical(label_list)\n",
    "print(Y.shape)\n",
    "model = multi_layer_perceptron()\n",
    "#optimizer → Adamを使用\n",
    "opt = Adam(lr=0.001)\n",
    "#compiling model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
    "#学習の実行\n",
    "# epochs 繰り返し学習させる回数\n",
    "model.fit(image_list,Y,nb_epoch=1500,batch_size = 100,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "{\"username\":\"\",\"version\":\"5.2\",\"session\":\"b7463fd38072216bd6aa3cb87148f5e3\",\"msg_id\":\"4d1b395fed69559e7dcb811044b9129e\",\"msg_type\":\"execute_request\",\"date\":\"2017-11-10T08:18:44.002624Z\"}\n",
      "2\n",
      "{}\n",
      "2\n",
      "{}\n",
      "995\n",
      "{\"silent\":false,\"store_history\":true,\"user_expressions\":{},\"allow_stdin\":true,\"stop_on_error\":true,\"code\":\"#テスト用ディレクトリ(./data/train/)の画像でチェック、正答率を表示する\\ntotal = 0\\nok_count = 0\\n\\nfor dir in os.listdir(\\\"data/test\\\"):\\n  if dir == \\\".DS_Store\\\":\\n    continue\\n\\n  dir2 = \\\"data/test/\\\" + dir\\n  label = 0\\n\\n  if dir == \\\"onion\\\":\\n    label = 0\\n  elif dir == \\\"carot\\\":\\n    label = 1\\n\\n  for file in os.listdir(dir2):\\n    if file != \\\".DS_Store\\\":\\n      label_list.append(label)\\n      filepath = dir2 + \\\"/\\\" + file\\n      image = np.array(Image.open(filepath).resize((25,25)))\\n      print(filepath)\\n      image = image.transpose(2,0,1)\\n      image = image.reshape(1,image.shape[0] * image.shape[1] * image.shape[2]).astype(\\\"float32\\\")[0]\\n      result = model.predict_classes(np.array([image/255.]))\\n      print(\\\"label:\\\",label,\\\"result\\\",result[0])\\n\\n      total += 1.\\n\\n      if label == result[0]:\\n        ok_count += 1.\\n\\nprint(\\\"seikai:\\\",ok_count / total * 100,\\\"%\\\")\\n\"}\n",
      "data/test/carot/carot0.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8027d937d0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#テスト用ディレクトリ(./data/train/)の画像でチェック、正答率を表示する\n",
    "total = 0\n",
    "ok_count = 0\n",
    "\n",
    "for dir in os.listdir(\"data/test\"):\n",
    "  if dir == \".DS_Store\":\n",
    "    continue\n",
    "\n",
    "  dir2 = \"data/test/\" + dir\n",
    "  label = 0\n",
    "\n",
    "  if dir == \"onion\":\n",
    "    label = 0\n",
    "  elif dir == \"carot\":\n",
    "    label = 1\n",
    "\n",
    "  for file in os.listdir(dir2):\n",
    "    if file != \".DS_Store\":\n",
    "      label_list.append(label)\n",
    "      filepath = dir2 + \"/\" + file\n",
    "      image = np.array(Image.open(filepath).resize((25,25)))\n",
    "      print(filepath)\n",
    "      image = image.transpose(2,0,1)\n",
    "      image = image.reshape(1,image.shape[0] * image.shape[1] * image.shape[2]).astype(\"float32\")[0]\n",
    "      result = model.predict_classes(np.array([image/255.]))\n",
    "      print(\"label:\",label,\"result\",result[0])\n",
    "\n",
    "      total += 1.\n",
    "\n",
    "      if label == result[0]:\n",
    "        ok_count += 1.\n",
    "\n",
    "print(\"seikai:\",ok_count / total * 100,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-TensorFlow",
   "language": "python",
   "name": "py35_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
